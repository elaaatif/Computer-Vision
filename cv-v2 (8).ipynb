{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":246729,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":210849,"modelId":232538},{"sourceId":246733,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":210853,"modelId":232542}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#4c1a6f; padding:20px; text-align:center;\">\n    <h1 style=\"color:white;\">FASTER RCNN MODEL FOR FIRE DETECTIONN</h1>\n       <h5 style=\"color:white;\">USING SATELITE IMAGES FOR OBEJCT DETECTION - A COMPUTER VISION PROJECT</hdiv>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#008093; padding:10px; text-align:center;\">\n    <h4 style=\"color:white;\">PIP INSTALLS </h4>\n       <h5 style=\"color:white\">\">roboflow - for loading the datasets </h5>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install roboflow torch torchvision matplotlib numpy pycocotools\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:36:42.495885Z","iopub.execute_input":"2025-01-31T20:36:42.496177Z","iopub.status.idle":"2025-01-31T20:36:47.293968Z","shell.execute_reply.started":"2025-01-31T20:36:42.496153Z","shell.execute_reply":"2025-01-31T20:36:47.293175Z"}},"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.53-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.12.14)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\nCollecting python-dotenv (from roboflow)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nDownloading roboflow-1.1.53-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: filetype, python-dotenv, idna, roboflow\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.53\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"> https://universe.roboflow.com/testes-jnrhu/wildfire-n12rk/dataset/1#","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x640\nAugmentations\nNo augmentations were applied.","metadata":{}},{"cell_type":"code","source":"%pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"testes-jnrhu\").project(\"wildfire-n12rk\")\nversion = project.version(1)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:36:47.295062Z","iopub.execute_input":"2025-01-31T20:36:47.295264Z","iopub.status.idle":"2025-01-31T20:36:55.988447Z","shell.execute_reply.started":"2025-01-31T20:36:47.295245Z","shell.execute_reply":"2025-01-31T20:36:55.987730Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.53)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.12.14)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\nRequirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nRequirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nloading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in wildfire-1 to yolov8-obb:: 100%|██████████| 27353/27353 [00:02<00:00, 12783.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to wildfire-1 in yolov8-obb:: 100%|██████████| 1012/1012 [00:00<00:00, 7344.11it/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"> https://universe.roboflow.com/bairock/wildfire-satellite/dataset/2#","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 416x416\nAugmentations\nOutputs per training example: 3\nFlip: Horizontal, Vertical","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"bairock\").project(\"wildfire-satellite\")\nversion = project.version(2)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:36:55.989970Z","iopub.execute_input":"2025-01-31T20:36:55.990175Z","iopub.status.idle":"2025-01-31T20:37:08.974337Z","shell.execute_reply.started":"2025-01-31T20:36:55.990157Z","shell.execute_reply":"2025-01-31T20:37:08.973610Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Wildfire-Satellite-2 to yolov8-obb:: 100%|██████████| 239413/239413 [00:07<00:00, 30834.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Wildfire-Satellite-2 in yolov8-obb:: 100%|██████████| 18065/18065 [00:01<00:00, 9306.15it/s] \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"> https://universe.roboflow.com/mini-z0ruz/wildfire-yh86l/dataset/2#","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x640\nAugmentations\nNo augmentations were applied.","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"mini-z0ruz\").project(\"wildfire-yh86l\")\nversion = project.version(2)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:08.975551Z","iopub.execute_input":"2025-01-31T20:37:08.975757Z","iopub.status.idle":"2025-01-31T20:37:20.190715Z","shell.execute_reply.started":"2025-01-31T20:37:08.975739Z","shell.execute_reply":"2025-01-31T20:37:20.189936Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Wildfire-2 to yolov8-obb:: 100%|██████████| 215379/215379 [00:07<00:00, 30356.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Wildfire-2 in yolov8-obb:: 100%|██████████| 7990/7990 [00:01<00:00, 7334.50it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"> https://universe.roboflow.com/fireforestgen/fire_forest_new/dataset/8#","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nNo preprocessing steps were applied.\nAugmentations\nNo augmentations were applied.","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"fireforestgen\").project(\"fire_forest_new\")\nversion = project.version(8)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:20.191730Z","iopub.execute_input":"2025-01-31T20:37:20.192059Z","iopub.status.idle":"2025-01-31T20:37:26.621318Z","shell.execute_reply.started":"2025-01-31T20:37:20.192028Z","shell.execute_reply":"2025-01-31T20:37:26.620495Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in fire_forest_new-8 to yolov8-obb:: 100%|██████████| 80043/80043 [00:03<00:00, 24449.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to fire_forest_new-8 in yolov8-obb:: 100%|██████████| 1045/1045 [00:00<00:00, 3989.70it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"> https://universe.roboflow.com/fireforestgen/fire_forest_gen/dataset/8","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nResize: Stretch to 800x800\nAugmentations\nNo augmentations were applied.","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"fireforestgen\").project(\"fire_forest_gen\")\nversion = project.version(8)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:26.622192Z","iopub.execute_input":"2025-01-31T20:37:26.622523Z","iopub.status.idle":"2025-01-31T20:37:31.006506Z","shell.execute_reply.started":"2025-01-31T20:37:26.622500Z","shell.execute_reply":"2025-01-31T20:37:31.005606Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in fire_forest_gen-8 to yolov8-obb:: 100%|██████████| 39471/39471 [00:01<00:00, 19762.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to fire_forest_gen-8 in yolov8-obb:: 100%|██████████| 985/985 [00:00<00:00, 6299.95it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"> https://universe.roboflow.com/12345-su5ee/-c956b/dataset/1#","metadata":{}},{"cell_type":"markdown","source":"Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x640\nAugmentations\nNo augmentations were applied.","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"12345-su5ee\").project(\"-c956b\")\nversion = project.version(1)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:31.007468Z","iopub.execute_input":"2025-01-31T20:37:31.007691Z","iopub.status.idle":"2025-01-31T20:37:35.470077Z","shell.execute_reply.started":"2025-01-31T20:37:31.007671Z","shell.execute_reply":"2025-01-31T20:37:35.469333Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Хакатон-Пожары-1 to yolov8-obb:: 100%|██████████| 7923/7923 [00:00<00:00, 8301.50it/s] ","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Хакатон-Пожары-1 in yolov8-obb:: 100%|██████████| 240/240 [00:00<00:00, 6497.21it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"> https://universe.roboflow.com/davidturati/640x640-xdhu3/dataset/24#","metadata":{}},{"cell_type":"markdown","source":"\n- Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x6* \n4- 0\nAugmentations\nOutputs per training example: 3\nFlip: Horizontal, Vertical\n90° Rotate: Clockwise, Counter-Clockwise, Upside Down\nCrop: 0% Minimum Zoom, 20% Maximum Zoom\nGrayscale: Apply to 15% of images\nHue: Between -15° and +15°\nSaturation: Between -25% and +25%\nBrightness: Between -15% and +15%\nExposure: Between -10% and +10%\nNoise: Up to 0.1% of pixels\nCutout: 3 boxes with0% size each\n*","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"davidturati\").project(\"640x640-xdhu3\")\nversion = project.version(24)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:35.472334Z","iopub.execute_input":"2025-01-31T20:37:35.472549Z","iopub.status.idle":"2025-01-31T20:37:45.010260Z","shell.execute_reply.started":"2025-01-31T20:37:35.472530Z","shell.execute_reply":"2025-01-31T20:37:45.009461Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in 640x640-24 to yolov8-obb:: 100%|██████████| 167436/167436 [00:05<00:00, 29659.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to 640x640-24 in yolov8-obb:: 100%|██████████| 7138/7138 [00:00<00:00, 7735.90it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"> https://universe.roboflow.com/pfc-dshky/fire-detection2-wayva/dataset/1","metadata":{}},{"cell_type":"markdown","source":"- Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x6\n4- 0\nAugmentations\nOutputs per training example: 3\nFlip: Horizontal, Vertical\n90° Rotate: Clockwise, Counter-Clockwise, Upside Down","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"pfc-dshky\").project(\"fire-detection2-wayva\")\nversion = project.version(1)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:45.012058Z","iopub.execute_input":"2025-01-31T20:37:45.012262Z","iopub.status.idle":"2025-01-31T20:37:51.809262Z","shell.execute_reply.started":"2025-01-31T20:37:45.012243Z","shell.execute_reply":"2025-01-31T20:37:51.808526Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in fire-detection2-1 to yolov8-obb:: 100%|██████████| 79566/79566 [00:03<00:00, 25896.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to fire-detection2-1 in yolov8-obb:: 100%|██████████| 5788/5788 [00:00<00:00, 9757.29it/s] \n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"> https://universe.roboflow.com/htw-berlin-xv7eo/dlr/dataset/1","metadata":{}},{"cell_type":"markdown","source":"* Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 1000x100* 0\nAugmentations\nOutputs per training example: 3\n90° Rotate: Clockwise, Counter-Clockwise, Upside Down\nCrop: 0% Minimum Zoom, 32% Maximum Zoom\nHue: Between -25° and +25°\nSaturation: Between -52% and +52%\nExposure: Between -25% and +25%\nBlur: Up to 1.75px\nNoise: Up to 5% of pixels * ","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"htw-berlin-xv7eo\").project(\"dlr\")\nversion = project.version(1)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:37:51.810107Z","iopub.execute_input":"2025-01-31T20:37:51.810313Z","iopub.status.idle":"2025-01-31T20:38:06.005085Z","shell.execute_reply.started":"2025-01-31T20:37:51.810295Z","shell.execute_reply":"2025-01-31T20:38:06.004509Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in DLR-1 to yolov8-obb:: 100%|██████████| 270176/270176 [00:09<00:00, 27970.82it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to DLR-1 in yolov8-obb:: 100%|██████████| 2398/2398 [00:00<00:00, 3162.52it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"> https://universe.roboflow.com/di-gao/volcano-nztt3/dataset/19","metadata":{}},{"cell_type":"markdown","source":"- Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 416x41- 6\nAugmentations\nOutputs per training example: 3\nFlip: Horizontal, Vertical\n90° Rotate: Clockwise, Counter-Clockwise, Upside Down\nBlur: Up to 1px\nNoise: Up to 5% of pixels","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"di-gao\").project(\"volcano-nztt3\")\nversion = project.version(19)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:06.005884Z","iopub.execute_input":"2025-01-31T20:38:06.006168Z","iopub.status.idle":"2025-01-31T20:38:10.185066Z","shell.execute_reply.started":"2025-01-31T20:38:06.006139Z","shell.execute_reply":"2025-01-31T20:38:10.184310Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in volcano-19 to yolov8-obb:: 100%|██████████| 17054/17054 [00:01<00:00, 13497.79it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to volcano-19 in yolov8-obb:: 100%|██████████| 612/612 [00:00<00:00, 7516.32it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"> https://universe.roboflow.com/umbc-s6g8c/wildfire-detection-satellite/dataset/3","metadata":{}},{"cell_type":"markdown","source":"- Preprocessing\nAuto-Orient: Applied\nResize: Stretch to 640x640\nTile: 2 rows x 2 colu -m- ns\nAugmentations\nOutputs per training example: 3\nFlip: Horizontal, Vertical\nRotation: Between -15° and +15°\nSaturation: Between -25% and +25%","metadata":{}},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"7IyVahMNGQIrl0do1Zz8\")\nproject = rf.workspace(\"umbc-s6g8c\").project(\"wildfire-detection-satellite\")\nversion = project.version(3)\ndataset = version.download(\"yolov8-obb\")\n                ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:10.185840Z","iopub.execute_input":"2025-01-31T20:38:10.186072Z","iopub.status.idle":"2025-01-31T20:38:43.601672Z","shell.execute_reply.started":"2025-01-31T20:38:10.186042Z","shell.execute_reply":"2025-01-31T20:38:43.600963Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Wildfire-Detection-Satellite-3 to yolov8-obb:: 100%|██████████| 773938/773938 [00:23<00:00, 32682.55it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Wildfire-Detection-Satellite-3 in yolov8-obb:: 100%|██████████| 60972/60972 [00:06<00:00, 9841.29it/s] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# import os\n\n# class UniqueLabelsExtractor:\n#     def __init__(self, labels_dir):\n#         self.labels_dir = labels_dir\n\n#     def get_unique_labels(self):\n#         unique_labels = set()\n\n#         label_files = os.listdir(self.labels_dir)\n#         for label_file in label_files:\n#             label_path = os.path.join(self.labels_dir, label_file)\n#             with open(label_path, 'r', encoding='latin1') as f:\n#                 for line in f:\n#                     parts = line.strip().split()\n#                     if not parts or len(parts) < 5:\n#                         continue  # Skip empty or invalid lines\n#                     label = int(parts[0])  # Adjusting label index\n#                     unique_labels.add(label)\n\n#         return sorted(unique_labels)\n\n# # Example usage:\n# labels_dir = \"/kaggle/working/combined_dataset/train/labels\"\n# extractor = UniqueLabelsExtractor(labels_dir)\n# unique_labels = extractor.get_unique_labels()\n# print(\"Unique Labels:\", unique_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:43.602625Z","iopub.execute_input":"2025-01-31T20:38:43.602908Z","iopub.status.idle":"2025-01-31T20:38:43.606324Z","shell.execute_reply.started":"2025-01-31T20:38:43.602879Z","shell.execute_reply":"2025-01-31T20:38:43.605663Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\n\nclass UniqueLabelsExtractor:\n    def __init__(self, labels_dir):\n        self.labels_dir = labels_dir\n\n    def get_unique_labels(self):\n        unique_labels = set()\n        \n        if not os.path.exists(self.labels_dir):\n            return unique_labels  # Return empty set if path doesn't exist\n\n        label_files = os.listdir(self.labels_dir)\n        for label_file in label_files:\n            label_path = os.path.join(self.labels_dir, label_file)\n            with open(label_path, 'r', encoding='latin1') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if not parts or len(parts) < 5:\n                        continue  # Skip empty or invalid lines\n                    label = int(parts[0])  # Adjusting label index\n                    unique_labels.add(label)\n        \n        return unique_labels\n\n# Define base path and dataset folders\nbase_path = \"/kaggle/working\"\nfolders = [\n    \"640x640-24\", \"DLR-1\", \"Wildfire-2\", \"Wildfire-Detection-Satellite-3\",\n    \"Wildfire-Satellite-2\", \"fire-detection2-1\", \"fire_forest_gen-8\",\n    \"fire_forest_new-8\", \"volcano-19\", \"wildfire-1\", \"Хакатон-Пожары-1\",\"combined_dataset\"\n]\n\n# Iterate over each dataset folder and extract unique labels\nfor folder in folders:\n    labels_dir = os.path.join(base_path, folder, \"train\", \"labels\")\n    extractor = UniqueLabelsExtractor(labels_dir)\n    unique_labels = extractor.get_unique_labels()\n    print(f\"Dataset: {folder}, Unique Labels Count: {len(unique_labels)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:43.607238Z","iopub.execute_input":"2025-01-31T20:38:43.607479Z","iopub.status.idle":"2025-01-31T20:38:44.751151Z","shell.execute_reply.started":"2025-01-31T20:38:43.607454Z","shell.execute_reply":"2025-01-31T20:38:44.750484Z"}},"outputs":[{"name":"stdout","text":"Dataset: 640x640-24, Unique Labels Count: 1\nDataset: DLR-1, Unique Labels Count: 1\nDataset: Wildfire-2, Unique Labels Count: 1\nDataset: Wildfire-Detection-Satellite-3, Unique Labels Count: 2\nDataset: Wildfire-Satellite-2, Unique Labels Count: 1\nDataset: fire-detection2-1, Unique Labels Count: 1\nDataset: fire_forest_gen-8, Unique Labels Count: 1\nDataset: fire_forest_new-8, Unique Labels Count: 1\nDataset: volcano-19, Unique Labels Count: 3\nDataset: wildfire-1, Unique Labels Count: 1\nDataset: Хакатон-Пожары-1, Unique Labels Count: 1\nDataset: combined_dataset, Unique Labels Count: 0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"<div style=\"background-color:#008093; padding:10px; text-align:center;\">\n       <h5 style=\"color:white;\">MERGING THE DATASETS</h5>\n    <h5 style=\"color:white;\">TRAIN - TEST - VALID</h5>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport shutil\n\nbase_path ='/kaggle/working'\nfolders = [\n    \"640x640-24\", \"DLR-1\", \"Wildfire-2\",\n    \"Wildfire-Satellite-2\", \"fire-detection2-1\", \"fire_forest_gen-8\",\n    \"fire_forest_new-8\", \"wildfire-1\", \"Хакатон-Пожары-1\"\n]\n\n# Check the folder structure of each dataset\nfor folder in folders:\n    dataset_path = os.path.join(base_path, folder)\n    print(f\"Contents of {folder}:\")\n    print(os.listdir(dataset_path))\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:44.751836Z","iopub.execute_input":"2025-01-31T20:38:44.752068Z","iopub.status.idle":"2025-01-31T20:38:44.762053Z","shell.execute_reply.started":"2025-01-31T20:38:44.752047Z","shell.execute_reply":"2025-01-31T20:38:44.761216Z"}},"outputs":[{"name":"stdout","text":"Contents of 640x640-24:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of DLR-1:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of Wildfire-2:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of Wildfire-Satellite-2:\n['README.dataset.txt', 'valid', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of fire-detection2-1:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of fire_forest_gen-8:\n['README.dataset.txt', 'valid', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of fire_forest_new-8:\n['README.dataset.txt', 'valid', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of wildfire-1:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\nContents of Хакатон-Пожары-1:\n['README.dataset.txt', 'valid', 'test', 'train', 'data.yaml', 'README.roboflow.txt']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Define combined dataset paths\ncombined_path = '/kaggle/working/combined_dataset'\n\ndef create_combined_folders(base):\n    paths = {}\n    for split in ['train', 'valid', 'test']:\n        images_path = os.path.join(base, split, 'images')\n        labels_path = os.path.join(base, split, 'labels')\n        os.makedirs(images_path, exist_ok=True)\n        os.makedirs(labels_path, exist_ok=True)\n        paths[split] = {'images': images_path, 'labels': labels_path}\n    return paths\ncombined_paths = create_combined_folders(combined_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:44.762860Z","iopub.execute_input":"2025-01-31T20:38:44.763157Z","iopub.status.idle":"2025-01-31T20:38:44.771776Z","shell.execute_reply.started":"2025-01-31T20:38:44.763125Z","shell.execute_reply":"2025-01-31T20:38:44.771149Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Function to merge image or label files\ndef merge_files(src_folder, dest_folder, folder_name):\n    if os.path.exists(src_folder):  # Check if the source folder exists\n        for filename in os.listdir(src_folder):\n            src_file = os.path.join(src_folder, filename)\n            new_filename = f\"{folder_name}_{filename}\"\n            dest_file = os.path.join(dest_folder, new_filename)\n\n            # Avoid overwriting by renaming duplicates\n            while os.path.exists(dest_file):\n                base, ext = os.path.splitext(new_filename)\n                new_filename = f\"{base}_copy{ext}\"\n                dest_file = os.path.join(dest_folder, new_filename)\n\n            shutil.move(src_file, dest_file)\n\n# Iterate through each dataset and move files\nfor folder in folders:\n    dataset_path = os.path.join(base_path, folder)\n\n    for split in ['train', 'valid', 'test']:\n        split_images = os.path.join(dataset_path, split, 'images')\n        split_labels = os.path.join(dataset_path, split, 'labels')\n\n        merge_files(split_images, combined_paths[split]['images'], folder)\n        merge_files(split_labels, combined_paths[split]['labels'], folder)\n\nprint(\"Datasets merged successfully with separate folders for images and labels in each split!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:44.772543Z","iopub.execute_input":"2025-01-31T20:38:44.772809Z","iopub.status.idle":"2025-01-31T20:38:46.139877Z","shell.execute_reply.started":"2025-01-31T20:38:44.772781Z","shell.execute_reply":"2025-01-31T20:38:46.139016Z"}},"outputs":[{"name":"stdout","text":"Datasets merged successfully with separate folders for images and labels in each split!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Verify the Combined Dataset\nfor folder in ['train', 'valid', 'test']:\n    folder_path = os.path.join(combined_path, folder)\n    images_path = os.path.join(folder_path, 'images')\n    labels_path = os.path.join(folder_path, 'labels')\n    \n    print(f\"\\n{folder.capitalize()} Folder:\")\n    \n    if os.path.exists(images_path):\n        num_images = len(os.listdir(images_path))\n        print(f\"  - Images folder contains {num_images} files.\")\n    else:\n        print(\"  - Images folder is missing.\")\n    \n    if os.path.exists(labels_path):\n        num_labels = len(os.listdir(labels_path))\n        print(f\"  - Labels folder contains {num_labels} files.\")\n    else:\n        print(\"  - Labels folder is missing.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:46.140779Z","iopub.execute_input":"2025-01-31T20:38:46.141099Z","iopub.status.idle":"2025-01-31T20:38:46.184158Z","shell.execute_reply.started":"2025-01-31T20:38:46.141067Z","shell.execute_reply":"2025-01-31T20:38:46.183334Z"}},"outputs":[{"name":"stdout","text":"\nTrain Folder:\n  - Images folder contains 19407 files.\n  - Labels folder contains 19407 files.\n\nValid Folder:\n  - Images folder contains 1623 files.\n  - Labels folder contains 1623 files.\n\nTest Folder:\n  - Images folder contains 1251 files.\n  - Labels folder contains 1251 files.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"<div style=\"background-color:#008093; padding:10px; text-align:center;\">\n       <h5 style=\"color:white;\">PREPING THE MODEL</h5>\n</div>","metadata":{"execution":{"iopub.execute_input":"2024-12-13T11:16:58.795750Z","iopub.status.busy":"2024-12-13T11:16:58.794445Z","iopub.status.idle":"2024-12-13T11:16:58.803723Z","shell.execute_reply":"2024-12-13T11:16:58.801441Z","shell.execute_reply.started":"2024-12-13T11:16:58.795695Z"}}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport os\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:46.185041Z","iopub.execute_input":"2025-01-31T20:38:46.185335Z","iopub.status.idle":"2025-01-31T20:38:51.361639Z","shell.execute_reply.started":"2025-01-31T20:38:46.185303Z","shell.execute_reply":"2025-01-31T20:38:51.360703Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, images_dir, labels_dir, transforms=None, max_samples=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.transforms = transforms\n        \n        # Get sorted file lists\n        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n        self.label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])\n        \n        # Verify matching files\n        assert len(self.image_files) == len(self.label_files), \"Mismatch between number of images and labels\"\n\n        # Apply max_samples limit\n        if max_samples is not None:\n            self.image_files = self.image_files[:max_samples]\n            self.label_files = self.label_files[:max_samples]\n\n    # Keep the rest of the class unchanged\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        img_width, img_height = img.size\n\n        # Load labels\n        label_path = os.path.join(self.labels_dir, self.label_files[idx])\n        boxes = []\n        labels = []\n        \n        try:\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) == 5:  # Ensure we have all 5 components\n                        label = int(parts[0])  # Don't add 1 here\n                        x_center, y_center, width, height = map(float, parts[1:5])\n\n                        width, height = 224,224\n                        \n                        # Skip invalid boxes\n                        if width <= 0 or height <= 0:\n                            continue\n                            \n                        # Convert YOLO to absolute coordinates\n                        x_min = (x_center - width/2) * img_width\n                        y_min = (y_center - height/2) * img_height\n                        x_max = (x_center + width/2) * img_width\n                        y_max = (y_center + height/2) * img_height\n                        \n                        # Ensure coordinates are within image bounds\n                        x_min = max(0, min(x_min, img_width))\n                        y_min = max(0, min(y_min, img_height))\n                        x_max = max(0, min(x_max, img_width))\n                        y_max = max(0, min(y_max, img_height))\n                        \n                        # Only add if box has valid dimensions\n                        if x_max > x_min and y_max > y_min:\n                            boxes.append([x_min, y_min, x_max, y_max])\n                            labels.append(label)\n        except Exception as e:\n            print(f\"Error reading label file {label_path}: {e}\")\n            return None\n\n        # Convert to tensors\n        if len(boxes) == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros(0, dtype=torch.int64)\n        else:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n        }\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:51.362358Z","iopub.execute_input":"2025-01-31T20:38:51.362728Z","iopub.status.idle":"2025-01-31T20:38:51.372693Z","shell.execute_reply.started":"2025-01-31T20:38:51.362705Z","shell.execute_reply":"2025-01-31T20:38:51.371779Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Define transforms\ndef get_transform(train):\n    transform_list = [\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),  # Convert image to tensor (0-1 range)\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n    ]\n    \n    if train:\n        transform_list.insert(0, transforms.RandomHorizontalFlip(p=0.5))  # Data augmentation for training\n\n    return transforms.Compose(transform_list)\n\n\n# Create the dataset with 5,000 samples\ntrain_dataset = CustomDataset(\n    images_dir='/kaggle/working/combined_dataset/train/images',\n    labels_dir='/kaggle/working/combined_dataset/train/labels',\n    transforms=get_transform(train=False),\n    max_samples=2000  # <-- Add this line\n)\n\n# # Create the dataset and dataloaders\n# train_dataset = CustomDataset(\n#     images_dir='/kaggle/working/combined_dataset/train/images',\n#     labels_dir='/kaggle/working/combined_dataset/train/labels',\n#     transforms=get_transform(train=True)\n# )\n\nvalid_dataset = CustomDataset(\n images_dir='/kaggle/working/combined_dataset/valid/images',\n labels_dir='/kaggle/working/combined_dataset/valid/labels',\n transforms=get_transform(train=False),\n  max_samples=2000  # <-- Add this line\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x) ))\nvalid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:43:42.733597Z","iopub.execute_input":"2025-01-31T20:43:42.733870Z","iopub.status.idle":"2025-01-31T20:43:42.800766Z","shell.execute_reply.started":"2025-01-31T20:43:42.733849Z","shell.execute_reply":"2025-01-31T20:43:42.800117Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"print(len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:43:43.046795Z","iopub.execute_input":"2025-01-31T20:43:43.047057Z","iopub.status.idle":"2025-01-31T20:43:43.051354Z","shell.execute_reply.started":"2025-01-31T20:43:43.047033Z","shell.execute_reply":"2025-01-31T20:43:43.050543Z"}},"outputs":[{"name":"stdout","text":"125\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# resnet101\n","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained model and modify it for the custom dataset\nnum_classes = 2  # Update this if you have more classes, 1 for background + N classes\nbackbone = torchvision.models.resnet101(pretrained=True)\nbackbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n\n# Freeze backbone layers (ADD THIS HERE)\nfor param in backbone.parameters():\n    param.requires_grad = False  # Freezes the backbone\n\nbackbone.out_channels = 2048\n\nanchor_generator = AnchorGenerator(\n    sizes=((32, 64, 128, 256, 512),),\n    aspect_ratios=((0.5, 1.0, 2.0),)\n)\n\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(\n    featmap_names=['0'], output_size=7, sampling_ratio=2\n)\n\nmodel = FasterRCNN(\n    backbone,\n    num_classes=num_classes,\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=roi_pooler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:44:24.502984Z","iopub.execute_input":"2025-01-31T20:44:24.503297Z","iopub.status.idle":"2025-01-31T20:44:26.748534Z","shell.execute_reply.started":"2025-01-31T20:44:24.503271Z","shell.execute_reply":"2025-01-31T20:44:26.747823Z"}},"outputs":[{"name":"stderr","text":"The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# EFFICIENT NET","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torchvision\n# from torchvision.models.detection import FasterRCNN\n# from torchvision.models.detection.rpn import AnchorGenerator\n# from torchvision.models import efficientnet_b3\n\n# # Load EfficientNet-B3 and modify it\n# num_classes = 2  # Update this if needed\n\n# # Load the pre-trained EfficientNet model\n# efficientnet = efficientnet_b3(weights=\"IMAGENET1K_V1\")\n# backbone = efficientnet.features  # Extract feature extractor\n\n# # Freeze backbone layers\n# for param in backbone.parameters():\n#     param.requires_grad = False\n\n# # Define the number of output channels (last feature map size of EfficientNet-B3)\n# backbone.out_channels = 1536  # EfficientNet-B3 last conv layer output channels\n\n# # Define anchor generator\n# anchor_generator = AnchorGenerator(\n#     sizes=((32, 64, 128, 256, 512),),\n#     aspect_ratios=((0.5, 1.0, 2.0),)\n# )\n\n# # Define ROI Pooler\n# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n#     featmap_names=['0'], output_size=7, sampling_ratio=2\n# )\n\n# # Create Faster R-CNN model\n# model = FasterRCNN(\n#     backbone,\n#     num_classes=num_classes,\n#     rpn_anchor_generator=anchor_generator,\n#     box_roi_pool=roi_pooler\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:54.776468Z","iopub.execute_input":"2025-01-31T20:38:54.776689Z","iopub.status.idle":"2025-01-31T20:38:54.780233Z","shell.execute_reply.started":"2025-01-31T20:38:54.776671Z","shell.execute_reply":"2025-01-31T20:38:54.779351Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# FASTER RCNN resnet50","metadata":{}},{"cell_type":"code","source":"# # Load a pre-trained model and modify it for the custom dataset\n# num_classes = 2  # Update this if you have more classes, 1 for background + N classes\n# backbone = torchvision.models.resnet50(pretrained=True)\n# backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n\n# # Freeze backbone layers (ADD THIS HERE)\n# for param in backbone.parameters():\n#     param.requires_grad = False  # Freezes the backbone\n\n# backbone.out_channels = 2048\n\n# anchor_generator = AnchorGenerator(\n#     sizes=((32, 64, 128, 256, 512),),\n#     aspect_ratios=((0.5, 1.0, 2.0),)\n# )\n\n# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n#     featmap_names=['0'], output_size=7, sampling_ratio=2\n# )\n\n# model = FasterRCNN(\n#     backbone,\n#     num_classes=num_classes,\n#     rpn_anchor_generator=anchor_generator,\n#     box_roi_pool=roi_pooler\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:54.780990Z","iopub.execute_input":"2025-01-31T20:38:54.781186Z","iopub.status.idle":"2025-01-31T20:38:57.226211Z","shell.execute_reply.started":"2025-01-31T20:38:54.781166Z","shell.execute_reply":"2025-01-31T20:38:57.225485Z"}},"outputs":[{"name":"stderr","text":"Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 214MB/s]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Define the optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n# Directory for saving history\nhistory_dir = \"/kaggle/working/history\"\nos.makedirs(history_dir, exist_ok=True)\n# History tracking\nhistory = {\"epochs\": []}\nimport torch\nfrom torchvision.ops import box_iou\nimport json\nimport os\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef calculate_metrics(pred_boxes, pred_labels, pred_scores, target_boxes, target_labels, iou_threshold=0.5):\n    \"\"\"Calculate object detection metrics for a batch of predictions.\"\"\"\n    batch_iou = 0.0\n    batch_precision = 0.0\n    batch_recall = 0.0\n    batch_f1 = 0.0\n    batch_map = 0.0  # mean Average Precision\n    \n    for pred_box, pred_label, pred_score, target_box, target_label in zip(\n        pred_boxes, pred_labels, pred_scores, target_boxes, target_labels):\n        \n        if len(pred_box) == 0 or len(target_box) == 0:\n            continue\n            \n        # Calculate IoU between predicted and target boxes\n        iou_matrix = box_iou(pred_box, target_box)\n        \n        # Calculate metrics\n        matches = iou_matrix > iou_threshold\n        true_positives = matches.sum().item()\n        false_positives = len(pred_box) - true_positives\n        false_negatives = len(target_box) - true_positives\n        \n        # IoU\n        if matches.numel() > 0:\n            batch_iou += iou_matrix[matches].mean().item()\n        \n        # Precision, Recall, F1\n        if true_positives + false_positives > 0:\n            precision = true_positives / (true_positives + false_positives)\n            batch_precision += precision\n        if true_positives + false_negatives > 0:\n            recall = true_positives / (true_positives + false_negatives)\n            batch_recall += recall\n        if precision + recall > 0:\n            f1 = 2 * (precision * recall) / (precision + recall)\n            batch_f1 += f1\n            \n        # Calculate mAP\n        # Sort predictions by confidence score\n        sorted_indices = torch.argsort(pred_score, descending=True)\n        pred_box = pred_box[sorted_indices]\n        pred_label = pred_label[sorted_indices]\n        pred_score = pred_score[sorted_indices]\n        \n        # Calculate AP for each class\n        unique_classes = torch.unique(torch.cat([pred_label, target_label]))\n        ap_sum = 0.0\n        \n        for cls in unique_classes:\n            cls_pred_mask = pred_label == cls\n            cls_target_mask = target_label == cls\n            \n            if cls_pred_mask.sum() == 0 or cls_target_mask.sum() == 0:\n                continue\n                \n            cls_iou_matrix = box_iou(pred_box[cls_pred_mask], target_box[cls_target_mask])\n            cls_matches = cls_iou_matrix > iou_threshold\n            \n            # Calculate precision at each threshold\n            precisions = []\n            for k in range(len(cls_matches)):\n                matches_k = cls_matches[:k+1]\n                if matches_k.numel() > 0:\n                    precisions.append(matches_k.sum().float() / (k + 1))\n            \n            if precisions:\n                ap_sum += torch.tensor(precisions).mean().item()\n        \n        if len(unique_classes) > 0:\n            batch_map += ap_sum / len(unique_classes)\n    \n    # Calculate batch averages\n    num_images = len(pred_boxes)\n    return {\n        'iou': batch_iou / num_images,\n        'precision': batch_precision / num_images,\n        'recall': batch_recall / num_images,\n        'f1': batch_f1 / num_images,\n        'map': batch_map / num_images\n    }\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, history_dir):\n    model.train()\n    total_loss = 0.0\n    total_iou = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    total_f1 = 0.0\n    total_map = 0.0\n    \n    for batch_idx, (images, targets) in enumerate(data_loader):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Forward pass and loss calculation\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        total_loss += losses.item()\n        \n        # Get predictions by temporarily setting model to eval mode\n        model.eval()\n        with torch.no_grad():\n            predictions = model(images)\n        model.train()  # Set back to training mode\n        \n        # Calculate metrics for the batch\n        batch_metrics = calculate_metrics(\n            [p['boxes'] for p in predictions],\n            [p['labels'] for p in predictions],\n            [p['scores'] for p in predictions],\n            [t['boxes'] for t in targets],\n            [t['labels'] for t in targets]\n        )\n        \n        total_iou += batch_metrics['iou']\n        total_precision += batch_metrics['precision']\n        total_recall += batch_metrics['recall']\n        total_f1 += batch_metrics['f1']\n        total_map += batch_metrics['map']\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n \n    # Calculate averages\n    num_batches = len(data_loader)\n    avg_loss = total_loss / num_batches\n    avg_iou = total_iou / num_batches\n    avg_precision = total_precision / num_batches\n    avg_recall = total_recall / num_batches\n    avg_f1 = total_f1 / num_batches\n    avg_map = total_map / num_batches\n    \n    # Print metrics\n    print(f\"\\nEpoch {epoch + 1} Summary:\")\n    print(f\"  Loss: {avg_loss:.4f}\")\n    print(f\"  IoU: {avg_iou:.4f}\")\n    print(f\"  Precision: {avg_precision:.4f}\")\n    print(f\"  Recall: {avg_recall:.4f}\")\n    print(f\"  F1-Score: {avg_f1:.4f}\")\n    print(f\"  mAP: {avg_map:.4f}\")\n    \n    # Save history\n    history = {\n        \"epoch\": epoch + 1,\n        \"loss\": avg_loss,\n        \"iou\": avg_iou,\n        \"precision\": avg_precision,\n        \"recall\": avg_recall,\n        \"f1\": avg_f1,\n        \"map\": avg_map\n    }\n    \n    # Ensure history directory exists\n    os.makedirs(history_dir, exist_ok=True)\n    \n    # Save to JSON file\n    history_file = os.path.join(history_dir, \"training_history.json\")\n    if os.path.exists(history_file):\n        with open(history_file, 'r') as f:\n            history_data = json.load(f)\n            history_data['epochs'].append(history)\n    else:\n        history_data = {'epochs': [history]}\n    \n    with open(history_file, 'w') as f:\n        json.dump(history_data, f, indent=4)\n    \n    return avg_loss, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:38:57.227431Z","iopub.execute_input":"2025-01-31T20:38:57.227768Z","iopub.status.idle":"2025-01-31T20:38:57.848159Z","shell.execute_reply.started":"2025-01-31T20:38:57.227729Z","shell.execute_reply":"2025-01-31T20:38:57.847258Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch,history_dir)\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:44:33.076893Z","iopub.execute_input":"2025-01-31T20:44:33.077177Z","iopub.status.idle":"2025-01-31T21:14:35.778734Z","shell.execute_reply.started":"2025-01-31T20:44:33.077156Z","shell.execute_reply":"2025-01-31T21:14:35.777537Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1 Summary:\n  Loss: 1.4060\n  IoU: 0.0000\n  Precision: 0.0000\n  Recall: 0.0000\n  F1-Score: 0.0000\n  mAP: 0.0000\n\nEpoch 2 Summary:\n  Loss: 1.4058\n  IoU: 0.0000\n  Precision: 0.0000\n  Recall: 0.0000\n  F1-Score: 0.0000\n  mAP: 0.0000\n\nEpoch 3 Summary:\n  Loss: 1.4058\n  IoU: 0.0000\n  Precision: 0.0000\n  Recall: 0.0000\n  F1-Score: 0.0000\n  mAP: 0.0000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-40c55471b958>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-5c27c39d2f4b>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, history_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Forward pass and loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# the proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mrel_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox_sum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36mdecode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Distance from center to box's corner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mc_to_c_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mc_to_c_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ctr_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":41},{"cell_type":"markdown","source":"# TEST AND VALIDATION ","metadata":{}},{"cell_type":"code","source":"# Validation function\ndef validate_model(model, data_loader, device):\n    model.eval()\n    total_loss = 0.0\n    total_iou = 0.0\n    total_precision = 0.0\n    total_recall = 0.0\n    \n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            total_loss += losses.item()\n            \n            # Dummy values for IoU, Precision, Recall (replace with actual calculations)\n            iou = torch.rand(1).item()\n            precision = torch.rand(1).item()\n            recall = torch.rand(1).item()\n            \n            total_iou += iou\n            total_precision += precision\n            total_recall += recall\n    \n    avg_loss = total_loss / len(data_loader)\n    avg_iou = total_iou / len(data_loader)\n    avg_precision = total_precision / len(data_loader)\n    avg_recall = total_recall / len(data_loader)\n    \n    print(f\"Validation: Avg Loss: {avg_loss:.4f}, IoU: {avg_iou:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}\")\n\n# Test function\ndef test_model(model, data_loader, device):\n    print(\"Testing model...\")\n    validate_model(model, data_loader, device)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.548376Z","iopub.status.idle":"2025-01-31T20:42:03.548701Z","shell.execute_reply":"2025-01-31T20:42:03.548584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.549651Z","iopub.status.idle":"2025-01-31T20:42:03.550107Z","shell.execute_reply":"2025-01-31T20:42:03.549909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\ndef plot_training_history(history):\n    epochs = [epoch[\"epoch\"] for epoch in history[\"epochs\"]]\n    losses = [epoch[\"loss\"] for epoch in history[\"epochs\"]]\n    ious = [epoch[\"iou\"] for epoch in history[\"epochs\"]]\n    precisions = [epoch[\"precision\"] for epoch in history[\"epochs\"]]\n    recalls = [epoch[\"recall\"] for epoch in history[\"epochs\"]]\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, losses, label='Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, ious, label='IoU')\n    plt.plot(epochs, precisions, label='Precision')\n    plt.plot(epochs, recalls, label='Recall')\n    plt.xlabel('Epoch')\n    plt.ylabel('Metrics')\n    plt.title('Object Detection Metrics')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# Display the training history graph\nplot_training_history(history)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.550888Z","iopub.status.idle":"2025-01-31T20:42:03.551267Z","shell.execute_reply":"2025-01-31T20:42:03.551110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\ndef inference_on_image(model, image_path, device, confidence_threshold=0.5):\n    \"\"\"\n    Perform inference on a single image\n    \n    Args:\n        model: Trained object detection model\n        image_path (str): Path to the input image\n        device: Torch device (cuda/cpu)\n        confidence_threshold (float): Minimum confidence to keep predictions\n    \n    Returns:\n        Predictions for the input image\n    \"\"\"\n    # Load image and convert to tensor\n\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n    \n    # Prepare image tensor\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    \n    # Disable gradient computation\n    with torch.no_grad():\n        # Get predictions\n        predictions = model(image_tensor)\n    \n    # Filter predictions by confidence threshold\n    filtered_predictions = []\n    for pred in predictions:\n        # Filter boxes based on confidence\n        mask = pred['scores'] > confidence_threshold\n        filtered_pred = {\n            'boxes': pred['boxes'][mask],\n            'labels': pred['labels'][mask],\n            'scores': pred['scores'][mask]\n        }\n        filtered_predictions.append(filtered_pred)\n    \n    return filtered_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.552694Z","iopub.status.idle":"2025-01-31T20:42:03.553066Z","shell.execute_reply":"2025-01-31T20:42:03.552896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming model is already trained and loaded\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nresults = inference_on_image(model, '/kaggle/input/sss.jpg/other/default/1/_98296600_california_fires_nasa_976map_jpg.rf.2c484e24e326712fa97ac36f5bc3b899.jpg', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.553842Z","iopub.status.idle":"2025-01-31T20:42:03.554217Z","shell.execute_reply":"2025-01-31T20:42:03.554057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T20:42:03.555155Z","iopub.status.idle":"2025-01-31T20:42:03.555534Z","shell.execute_reply":"2025-01-31T20:42:03.555356Z"}},"outputs":[],"execution_count":null}]}